<<!-- Float the image to the right -->
<img src="https://github.com/revanthsai36/revanthsai36/blob/main/Profile.JPG.jpeg?raw=true" alt="Revanth Sai" width="150" align="right" style="margin-left: 20px; border-radius: 8px;" />

<h1>Hi, I'm Revanth! <a href="https://www.linkedin.com/in/rev-sai-/">I am a Data Engineer</a></h1>

ðŸ“§ revanthsai276@gmail.comâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒðŸ“ž +1 (602)-921-7089  

---

## ðŸ”¹ Professional Summary

- Data Engineer with 5+ years of strong expertise in designing and maintaining end-to-end ETL pipelines using Python across cloud and on-prem environments.
- Proficient in building scalable, reliable, and high-performance data pipelines for both batch and streaming workflows using tools like Apache Airflow, Azure Data Factory, and GCP Dataflow.
- Advanced knowledge of Python programming, including data manipulation with Pandas, NumPy, and integration with cloud services and APIs.
- Extensive experience in data modeling, transformation, and ingestion into modern cloud data warehouses like BigQuery, Snowflake, and legacy databases.
- Strong SQL and NoSQL skills, with experience in MySQL, MongoDB, and Cosmos DB, including query optimization and schema tuning.
- Well-versed in data quality frameworks, validations, checks, and exception handling across pipeline stages.
- Skilled in debugging complex data issues, automating monitoring, and documenting workflows.
- Collaborative problem solver with experience partnering with analysts, data scientists, and business stakeholders.
- Exposure to AI/ML use cases through collaboration on model-ready pipelines and academic AI specialization.

---

## ðŸ”¹ Technical Summary

**Languages**: Python, SQL, Shell Scripting, PySpark  
**ETL/Orchestration**: Apache Airflow, Azure Data Factory, GCP Dataflow, SSIS, Azure Synapse  
**Cloud Platforms**: GCP (BigQuery, Pub/Sub, Composer), Azure (Data Lake, Synapse), DataProc  
**Data Warehouses**: BigQuery, Snowflake, Teradata, Legacy Databases  
**Databases**: MongoDB, Cosmos DB, Cassandra, MySQL, Oracle  
**Big Data**: Databricks, Hadoop, Kafka, DataProc  
**Data Modeling**: Star/Snowflake Schema, ERD, Normalization, Partitioning  
**CI/CD & DevOps**: Git, GitHub, Jenkins, Docker, Azure DevOps  
**Compliance**: HIPAA, GDPR, Audit Documentation  
**Visualization**: Power BI, Tableau

---

## ðŸ”¹ Professional Experience

### ðŸ“Œ Data Engineer â€“ Verizon (Feb 2023 â€“ Present)
**Project: GCP-Based Customer & Operations Analytics Platform**

- Migrated 12+ Teradata ETL workflows to BigQuery, improving speed by 50% and reducing costs by 30%.
- Built real-time pipelines in GCP Dataflow to ingest 2M+ daily events.
- Orchestrated workflows using Airflow (Cloud Composer); reduced failure rates by 40%.
- Created Python-based data validation scripts, cutting QA time by 60%.
- Designed partitioned BigQuery tables enabling sub-second queries.
- Power BI dashboards:
  - **Churn Prediction**: Tracked 250K+ users for high-risk identification.
  - **Network Health**: Visualized outages across 150+ regions.
  - **Support Insights**: Analyzed sentiment from 1M+ support logs.
- Enabled CI/CD with Docker and GitHub Actions.
- Delivered model-ready datasets, improving ML performance by 20%.

**Tech**: Python, Airflow, BigQuery, GCP Dataflow, SSIS, SQL, Docker, Git, Power BI, MongoDB

---

### ðŸ“Œ Data Engineer â€“ Humana (Aug 2021 â€“ Sep 2022)  
**Project: Azure-Based Healthcare Data Modernization**

- Designed scalable pipelines in Azure Data Factory and Databricks.
- Used PySpark and Python scripts for transformation and anomaly detection.
- Built star-schema models in Synapse for performance reporting.
- Power BI dashboards:
  - **Care Utilization Tracker**
  - **Cost & Claims Insights**
  - **Chronic Risk Scoring**
- Ensured
